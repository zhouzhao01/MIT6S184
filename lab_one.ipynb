{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d49b57-85cb-407c-a2e3-c79f682a3dc1",
   "metadata": {},
   "source": [
    "# Lab One: Simulating ODEs and SDEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e99dea-acb4-4fad-a347-1b99567c3188",
   "metadata": {},
   "source": [
    "Welcome to lab one! In this lab, we will provide an intuitive and hands-on walk-through of ODEs and SDEs. If you find any mistakes, or have any other feedback, please feel free to email us at `erives@mit.edu` and `phold@mit.edu`. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe737f00-0dff-4ffe-b40a-8187f8c615b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.axes._axes import Axes\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "from torch.func import vmap, jacrev\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd11ba4-80a5-4354-913b-ae17827f59e1",
   "metadata": {},
   "source": [
    "# Part 0: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43565591-329c-43ba-85fe-620b5eb2219b",
   "metadata": {},
   "source": [
    "First, let us make precise the central objects of study: *ordinary differential equations* (ODEs) and *stochastic differential equations* (SDEs). The basis of both ODEs and SDEs are time-dependent *vector fields*, which we recall from lecture as being functions $u$ defined by $$u:\\mathbb{R}^d\\times [0,1]\\to \\mathbb{R}^d,\\quad (x,t)\\mapsto u_t(x)$$\n",
    "That is, $u_t(x)$ takes in *where in space we are* ($x$) and *where in time we are* ($t$), and spits out the *direction we should be going in* $u_t(x)$. An ODE is then given by $$d X_t = u_t(X_t)dt, \\quad \\quad X_0 = x_0.$$\n",
    "Similarly, an SDE is of the form $$d X_t = u_t(X_t)dt + \\sigma_t d W_t, \\quad \\quad X_0 = x_0,$$\n",
    "which can be thought of as starting with an ODE given by $u_t$, and adding noise via the *Brownian motion* $(W_t)_{0 \\le t \\le 1}$. The amount of noise that we add is given by the *diffusion coefficient* $\\sigma_t$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738edb40-d990-4ba6-8749-261c66ebe51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODE(ABC):\n",
    "    @abstractmethod\n",
    "    def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the drift coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (bs, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - drift_coefficient: shape (batch_size, dim)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class SDE(ABC):\n",
    "    @abstractmethod\n",
    "    def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the drift coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (batch_size, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - drift_coefficient: shape (batch_size, dim)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def diffusion_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the diffusion coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (batch_size, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - diffusion_coefficient: shape (batch_size, dim)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f02840-39af-41c0-b105-fdcc3aa3d5d2",
   "metadata": {},
   "source": [
    "**Note**: One might consider an ODE to be a special case of SDEs with zero diffusion coefficient. This intuition is valid, however for pedagogical (and performance) reasons, we will treat them separately for the scope of this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a220737-5fbb-4405-852d-a9610d27e345",
   "metadata": {},
   "source": [
    "# Part 1: Numerical Methods for Simulating ODEs and SDEs\n",
    "We may think of ODEs and SDEs as describing the motion of a particle through space. Intuitively, the ODE above says \"start at $X_0=x_0$\", and move so that your instantaneous velocity is given by $u_t(X_t)$. Similarly, the SDE says \"start at $X_0=x_0$\", and move so that your instantaneous velocity is given by $u_t(X_t)$ plus a little bit of random noise given scaled by $\\sigma_t$. Formally, these trajectories traced out by this intuitive descriptions are said to be *solutions* to the ODEs and SDEs, respectively. Numerical methods for computing these solutions are all essentially based on *simulating*, or *integrating*, the ODE or SDE. \n",
    "\n",
    "In this section we'll implement the *Euler* and *Euler-Maruyama* numerical simulation schemes for integrating ODEs and SDEs, respectively. Recall from lecture that the Euler simulation scheme corresponds to the discretization\n",
    "$$d X_t = u_t(X_t) dt  \\quad \\quad \\rightarrow \\quad \\quad X_{t + h} = X_t + hu_t(X_t),$$\n",
    "where $h = \\Delta t$ is the *step size*. Similarly, the Euler-Maruyama scheme corresponds to the discretization \n",
    "$$ dX_t = u(X_t,t) dt + \\sigma_t d W_t  \\quad \\quad \\rightarrow \\quad \\quad X_{t + h} = X_t + hu_t(X_t) + \\sqrt{h} \\sigma_t z_t, \\quad z_t \\sim N(0,I_d).$$ \n",
    "Let's implement these!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98202014-cf8e-4d50-a819-61d937ebea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, xt: torch.Tensor, t: torch.Tensor, dt: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Takes one simulation step\n",
    "        Args:\n",
    "            - xt: state at time t, shape (batch_size, dim)\n",
    "            - t: time, shape ()\n",
    "            - dt: time, shape ()\n",
    "        Returns:\n",
    "            - nxt: state at time t + dt\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def simulate(self, x: torch.Tensor, ts: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Simulates using the discretization gives by ts\n",
    "        Args:\n",
    "            - x_init: initial state at time ts[0], shape (batch_size, dim)\n",
    "            - ts: timesteps, shape (nts,)\n",
    "        Returns:\n",
    "            - x_final: final state at time ts[-1], shape (batch_size, dim)\n",
    "        \"\"\"\n",
    "        for t_idx in range(len(ts) - 1):\n",
    "            t = ts[t_idx]\n",
    "            h = ts[t_idx + 1] - ts[t_idx]\n",
    "            x = self.step(x, t, h)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def simulate_with_trajectory(self, x: torch.Tensor, ts: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Simulates using the discretization gives by ts\n",
    "        Args:\n",
    "            - x_init: initial state at time ts[0], shape (bs, dim)\n",
    "            - ts: timesteps, shape (num_timesteps,)\n",
    "        Returns:\n",
    "            - xs: trajectory of xts over ts, shape (batch_size, num_timesteps, dim)\n",
    "        \"\"\"\n",
    "        xs = [x.clone()]\n",
    "        for t_idx in tqdm(range(len(ts) - 1)):\n",
    "            t = ts[t_idx]\n",
    "            h = ts[t_idx + 1] - ts[t_idx]\n",
    "            x = self.step(x, t, h)\n",
    "            xs.append(x.clone())\n",
    "        return torch.stack(xs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9b6b5-e7eb-44a1-a180-aa55d16d8356",
   "metadata": {},
   "source": [
    "### Question 1.1: Implement EulerSimulator and EulerMaruyamaSimulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e5b471-a228-4688-903f-7a0898d8b736",
   "metadata": {},
   "source": [
    "**Your job**: Fill in the `step` methods of `EulerSimulator` and `EulerMaruyamaSimulator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326c993-b5ef-4b6c-97b5-a403a509b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EulerSimulator(Simulator):\n",
    "    def __init__(self, ode: ODE):\n",
    "        self.ode = ode\n",
    "        \n",
    "    def step(self, xt: torch.Tensor, t: torch.Tensor, h: torch.Tensor):\n",
    "        raise NotImplementedError(\"Fill me in for Question 1.1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151adb4-e013-4728-8c96-8bab07ee6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EulerMaruyamaSimulator(Simulator):\n",
    "    def __init__(self, sde: SDE):\n",
    "        self.sde = sde\n",
    "        \n",
    "    def step(self, xt: torch.Tensor, t: torch.Tensor, h: torch.Tensor):\n",
    "        raise NotImplementedError(\"Fill me in for Question 1.1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb930b6-63a1-4cc2-8f60-d30de5d380ea",
   "metadata": {},
   "source": [
    "**Note:** When the diffusion coefficient is zero, the Euler and Euler-Maruyama simulation are equivalent! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afdf59a-25e3-4537-a357-bbbb8ee5332b",
   "metadata": {},
   "source": [
    "# Part 2: Visualizing Solutions to SDEs\n",
    "Let's get a feel for what the solutions to these SDEs look like in practice (we'll get to ODEs later...). To do so, we we'll implement and visualize two special choices of SDEs from lecture: a (scaled) *Brownian motion*, and an *Ornstein-Uhlenbeck* (OU) process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c480e6-bfcf-4105-b21a-002d3a110923",
   "metadata": {},
   "source": [
    "### Question 2.1: Implementing Brownian Motion\n",
    "First, recall that a Brownian motion is recovered (by definition) by setting $u_t = 0$ and $\\sigma_t = \\sigma$, viz.,\n",
    "$$ dX_t = \\sigma dW_t, \\quad \\quad X_0 = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e83673-54dc-451e-8422-0110c462b295",
   "metadata": {},
   "source": [
    "**Your job**: Intuitively, what might be expect the trajectories of $X_t$ to look like when $\\sigma$ is very large? What about when $\\sigma$ is close to zero?\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba5c9e-f5cc-41a9-a850-43e46d79b3fb",
   "metadata": {},
   "source": [
    "**Your job**: Fill in the `drift_coefficient` and `difusion_coefficient` methods of the `BrownianMotion` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c372a79-585e-4bbb-a78e-3870dd5c458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrownianMotion(SDE):\n",
    "    def __init__(self, sigma: float):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the drift coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (bs, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - drift: shape (bs, dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Fill me in for Question 2.1!\")\n",
    "        \n",
    "    def diffusion_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the diffusion coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (bs, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - diffusion: shape (bs, dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Fill me in for Question 2.1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f2318e-ae09-426a-bf57-c44cd5074288",
   "metadata": {},
   "source": [
    "Now let's plot! We'll make use of the following utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577649b-2290-4308-9d5a-7ab614984b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories_1d(x0: torch.Tensor, simulator: Simulator, timesteps: torch.Tensor, ax: Optional[Axes] = None):\n",
    "        \"\"\"\n",
    "        Graphs the trajectories of a one-dimensional SDE with given initial values (x0) and simulation timesteps (timesteps).\n",
    "        Args:\n",
    "            - x0: state at time t, shape (num_trajectories, 1)\n",
    "            - simulator: Simulator object used to simulate\n",
    "            - t: timesteps to simulate along, shape (num_timesteps,)\n",
    "            - ax: pyplot Axes object to plot on\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "        trajectories = simulator.simulate_with_trajectory(x0, timesteps) # (num_trajectories, num_timesteps, ...)\n",
    "        for trajectory_idx in range(trajectories.shape[0]):\n",
    "            trajectory = trajectories[trajectory_idx, :, 0] # (num_timesteps,)\n",
    "            ax.plot(ts.cpu(), trajectory.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4245ac9-3092-4c5b-a149-75a909ceaa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1.0\n",
    "brownian_motion = BrownianMotion(sigma)\n",
    "simulator = EulerMaruyamaSimulator(sde=brownian_motion)\n",
    "x0 = torch.zeros(5,1).to(device) # Initial values - let's start at zero\n",
    "ts = torch.linspace(0.0,5.0,500).to(device) # simulation timesteps\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.gca()\n",
    "ax.set_title(r'Trajectories of Brownian Motion with $\\sigma=$' + str(sigma), fontsize=18)\n",
    "ax.set_xlabel(r'Time ($t$)', fontsize=18)\n",
    "ax.set_ylabel(r'$X_t$', fontsize=18)\n",
    "plot_trajectories_1d(x0, simulator, ts, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7bd5c-d558-45e0-b741-6de4d7003776",
   "metadata": {},
   "source": [
    "**Your job**: What happens when you vary the value of `sigma`?\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b22c81b-7ce9-4b84-82ab-fac03f741b03",
   "metadata": {},
   "source": [
    "### Question 2.2: Implementing an Ornstein-Uhlenbeck Process\n",
    "An OU process is given by setting $u_t(X_t) = - \\theta X_t$ and $\\sigma_t = \\sigma$, viz.,\n",
    "$$ dX_t = -\\theta X_t\\, dt + \\sigma\\, dW_t, \\quad \\quad X_0 = x_0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0cdb7d-f8bf-4826-9c32-8cb101376103",
   "metadata": {},
   "source": [
    "**Your job**: Intuitively, what would the trajectory of $X_t$ look like for a very small value of $\\theta$? What about a very large value of $\\theta$?\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12325951-709c-4486-9ea7-f4c22b3cc1ef",
   "metadata": {},
   "source": [
    "**Your job**: Fill in the `drift_coefficient` and `difusion_coefficient` methods of the `OUProcess` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d0e94-698c-4729-878e-3c2a9881dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUProcess(SDE):\n",
    "    def __init__(self, theta: float, sigma: float):\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the drift coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (bs, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - drift: shape (bs, dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Fill me in for Question 2.2!\")\n",
    "        \n",
    "    def diffusion_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the diffusion coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (bs, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - diffusion: shape (bs, dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Fill me in for Question 2.2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7dc249-826e-4a81-91f7-fa8fb17278c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try comparing multiple choices side-by-side\n",
    "thetas_and_sigmas = [\n",
    "    (0.25, 0.0),\n",
    "    (0.25, 0.25),\n",
    "    (0.25, 0.5),\n",
    "    (0.25, 1.0),\n",
    "]\n",
    "simulation_time = 20.0\n",
    "\n",
    "num_plots = len(thetas_and_sigmas)\n",
    "fig, axes = plt.subplots(1, num_plots, figsize=(8 * num_plots, 7))\n",
    "\n",
    "for idx, (theta, sigma) in enumerate(thetas_and_sigmas):\n",
    "    ou_process = OUProcess(theta, sigma)\n",
    "    simulator = EulerMaruyamaSimulator(sde=ou_process)\n",
    "    x0 = torch.linspace(-10.0,10.0,10).view(-1,1).to(device) # Initial values - let's start at zero\n",
    "    ts = torch.linspace(0.0,simulation_time,1000).to(device) # simulation timesteps\n",
    "\n",
    "    ax = axes[idx]\n",
    "    ax.set_title(f'Trajectories of OU Process with $\\\\sigma = ${sigma}, $\\\\theta = ${theta}', fontsize=15)\n",
    "    ax.set_xlabel(r'Time ($t$)', fontsize=15)\n",
    "    ax.set_ylabel(r'$X_t$', fontsize=15)\n",
    "    plot_trajectories_1d(x0, simulator, ts, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e0c7eb-0eb3-4eb3-a854-5cac18a85baa",
   "metadata": {},
   "source": [
    "**Your job**: What do you notice about the convergence of the solutions? Are they converging to a particular point? Or to a distribution? Your answer should be two *qualitative* sentences of the form: \"When ($\\theta$ or $\\sigma$) goes (up or down), we see...\".\n",
    "\n",
    "**Hint**: Pay close attention to the ratio $D \\triangleq \\frac{\\sigma^2}{2\\theta}$ (see the next few cells below!).\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31502fba-c582-4d67-8fdf-8d474604cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scaled_trajectories_1d(x0: torch.Tensor, simulator: Simulator, timesteps: torch.Tensor, time_scale: float, label: str, ax: Optional[Axes] = None):\n",
    "        \"\"\"\n",
    "        Graphs the trajectories of a one-dimensional SDE with given initial values (x0) and simulation timesteps (timesteps).\n",
    "        Args:\n",
    "            - x0: state at time t, shape (num_trajectories, 1)\n",
    "            - simulator: Simulator object used to simulate\n",
    "            - t: timesteps to simulate along, shape (num_timesteps,)\n",
    "            - time_scale: scalar by which to scale time\n",
    "            - label: self-explanatory\n",
    "            - ax: pyplot Axes object to plot on\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "        trajectories = simulator.simulate_with_trajectory(x0, timesteps) # (num_trajectories, num_timesteps, ...)\n",
    "        for trajectory_idx in range(trajectories.shape[0]):\n",
    "            trajectory = trajectories[trajectory_idx, :, 0] # (num_timesteps,)\n",
    "            ax.plot(ts.cpu() * time_scale, trajectory.cpu(), label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50c533-a8cf-4fc0-be0c-b5fd5c3cbc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try rescaling with time\n",
    "sigmas = [1.0, 2.0, 10.0]\n",
    "ds = [0.25, 1.0, 4.0] # sigma**2 / 2t\n",
    "simulation_time = 10.0\n",
    "\n",
    "fig, axes = plt.subplots(len(ds), len(sigmas), figsize=(8 * len(sigmas), 8 * len(ds)))\n",
    "axes = axes.reshape((len(ds), len(sigmas)))\n",
    "for d_idx, d in enumerate(ds):\n",
    "    for s_idx, sigma in enumerate(sigmas):\n",
    "        theta = sigma**2 / 2 / d\n",
    "        ou_process = OUProcess(theta, sigma)\n",
    "        simulator = EulerMaruyamaSimulator(sde=ou_process)\n",
    "        x0 = torch.linspace(-20.0,20.0,20).view(-1,1).to(device)\n",
    "        time_scale = sigma**2\n",
    "        ts = torch.linspace(0.0,simulation_time / time_scale,1000).to(device) # simulation timesteps\n",
    "        ax = axes[d_idx, s_idx]\n",
    "        plot_scaled_trajectories_1d(x0=x0, simulator=simulator, timesteps=ts, time_scale=time_scale, label=f'Sigma = {sigma}', ax=ax)\n",
    "        ax.set_title(f'OU Trajectories with Sigma={sigma}, Theta={theta}, D={d}')\n",
    "        ax.set_xlabel(f't / (sigma^2)')\n",
    "        ax.set_ylabel('X_t')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850111a6-30be-4265-b423-ed23671deaf0",
   "metadata": {},
   "source": [
    "**Your job**: What conclusion can we draw from the figure above? One qualitative sentence is fine. We'll revisit this in Section 3.2.\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49bcbdc-237c-4639-a447-b0c13f575a8d",
   "metadata": {},
   "source": [
    "# Part 3: Transforming Distributions with SDEs\n",
    "In the previous section, we observed how individual *points* are transformed by an SDE. Ultimately, we are interested in understanding how *distributions* are transformed by an SDE (or an ODE...). After all, our goal is to design ODEs and SDEs which transform a noisy distribution (such as the Gaussian $N(0, I_d)$), to the data distribution $p_{\\text{data}}$ of interest. In this section, we will visualize how distributions are transformed by a very particular family of SDEs: *Langevin dynamics*.\n",
    "\n",
    "First, let's define some distributions to play around with. In practice, there are two qualities one might hope a distribution to have:\n",
    "1. The first quality is that one can measure the *density* of a distribution $p(x)$. This ensures that we can compute the gradient $\\nabla \\log p(x)$ of the log density. This quantity is known as the *score* of $p$, and paints a picture of the local geometry of the distribution. Using the score, we will construct and simulate the *Langevin dynamics*, a family of SDEs which \"drive\" samples toward the distribution $\\pi$. In particular, the Langevin dynamics *preserve* the distribution $p(x)$. In Lecture 2, we will make this notion of driving more precise.\n",
    "2. The second quality is that we can draw samples from the distribution $p(x)$.\n",
    "For simple, toy distributions, such as Gaussians and simple mixture models, it is often true that both qualities are satisfied. For more complex choices of $p$, such as distributions over images, we can sample but cannot measure the density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b64f2-732a-4ea3-84a4-8f955ca64f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Density(ABC):\n",
    "    \"\"\"\n",
    "    Distribution with tractable density\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def log_density(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the log density at x.\n",
    "        Args:\n",
    "            - x: shape (batch_size, dim)\n",
    "        Returns:\n",
    "            - log_density: shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def score(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the score dx log density(x)\n",
    "        Args:\n",
    "            - x: (batch_size, dim)\n",
    "        Returns:\n",
    "            - score: (batch_size, dim)\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, ...)\n",
    "        score = vmap(jacrev(self.log_density))(x)  # (batch_size, 1, 1, 1, ...)\n",
    "        return score.squeeze((1, 2, 3))  # (batch_size, ...)\n",
    "\n",
    "class Sampleable(ABC):\n",
    "    \"\"\"\n",
    "    Distribution which can be sampled from\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def sample(self, num_samples: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the log density at x.\n",
    "        Args:\n",
    "            - num_samples: the desired number of samples\n",
    "        Returns:\n",
    "            - samples: shape (batch_size, dim)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805b3f8-f0ab-4bb0-a41a-4d97c65e24e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several plotting utility functions\n",
    "def hist2d_sampleable(sampleable: Sampleable, num_samples: int, ax: Optional[Axes] = None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    samples = sampleable.sample(num_samples) # (ns, 2)\n",
    "    ax.hist2d(samples[:,0].cpu(), samples[:,1].cpu(), **kwargs)\n",
    "\n",
    "def scatter_sampleable(sampleable: Sampleable, num_samples: int, ax: Optional[Axes] = None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    samples = sampleable.sample(num_samples) # (ns, 2)\n",
    "    ax.scatter(samples[:,0].cpu(), samples[:,1].cpu(), **kwargs)\n",
    "\n",
    "def imshow_density(density: Density, bins: int, scale: float, ax: Optional[Axes] = None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x = torch.linspace(-scale, scale, bins).to(device)\n",
    "    y = torch.linspace(-scale, scale, bins).to(device)\n",
    "    X, Y = torch.meshgrid(x, y)\n",
    "    xy = torch.stack([X.reshape(-1), Y.reshape(-1)], dim=-1)\n",
    "    density = density.log_density(xy).reshape(bins, bins).T\n",
    "    im = ax.imshow(density.cpu(), extent=[-scale, scale, -scale, scale], origin='lower', **kwargs)\n",
    "\n",
    "def contour_density(density: Density, bins: int, scale: float, ax: Optional[Axes] = None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x = torch.linspace(-scale, scale, bins).to(device)\n",
    "    y = torch.linspace(-scale, scale, bins).to(device)\n",
    "    X, Y = torch.meshgrid(x, y)\n",
    "    xy = torch.stack([X.reshape(-1), Y.reshape(-1)], dim=-1)\n",
    "    density = density.log_density(xy).reshape(bins, bins).T\n",
    "    im = ax.contour(density.cpu(), extent=[-scale, scale, -scale, scale], origin='lower', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498eb6cb-1261-4cc1-b1d0-281e5f73d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(torch.nn.Module, Sampleable, Density):\n",
    "    \"\"\"\n",
    "    Two-dimensional Gaussian. Is a Density and a Sampleable. Wrapper around torch.distributions.MultivariateNormal\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, cov):\n",
    "        \"\"\"\n",
    "        mean: shape (2,)\n",
    "        cov: shape (2,2)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mean\", mean)\n",
    "        self.register_buffer(\"cov\", cov)\n",
    "\n",
    "    @property\n",
    "    def distribution(self):\n",
    "        return D.MultivariateNormal(self.mean, self.cov, validate_args=False)\n",
    "\n",
    "    def sample(self, num_samples) -> torch.Tensor:\n",
    "        return self.distribution.sample((num_samples,))\n",
    "\n",
    "    def log_density(self, x: torch.Tensor):\n",
    "        return self.distribution.log_prob(x).view(-1, 1)\n",
    "\n",
    "class GaussianMixture(torch.nn.Module, Sampleable, Density):\n",
    "    \"\"\"\n",
    "    Two-dimensional Gaussian mixture model, and is a Density and a Sampleable. Wrapper around torch.distributions.MixtureSameFamily.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        means: torch.Tensor,  # nmodes x data_dim\n",
    "        covs: torch.Tensor,  # nmodes x data_dim x data_dim\n",
    "        weights: torch.Tensor,  # nmodes\n",
    "    ):\n",
    "        \"\"\"\n",
    "        means: shape (nmodes, 2)\n",
    "        covs: shape (nmodes, 2, 2)\n",
    "        weights: shape (nmodes, 1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nmodes = means.shape[0]\n",
    "        self.register_buffer(\"means\", means)\n",
    "        self.register_buffer(\"covs\", covs)\n",
    "        self.register_buffer(\"weights\", weights)\n",
    "\n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self.means.shape[1]\n",
    "\n",
    "    @property\n",
    "    def distribution(self):\n",
    "        return D.MixtureSameFamily(\n",
    "                mixture_distribution=D.Categorical(probs=self.weights, validate_args=False),\n",
    "                component_distribution=D.MultivariateNormal(\n",
    "                    loc=self.means,\n",
    "                    covariance_matrix=self.covs,\n",
    "                    validate_args=False,\n",
    "                ),\n",
    "                validate_args=False,\n",
    "            )\n",
    "\n",
    "    def log_density(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.distribution.log_prob(x).view(-1, 1)\n",
    "\n",
    "    def sample(self, num_samples: int) -> torch.Tensor:\n",
    "        return self.distribution.sample(torch.Size((num_samples,)))\n",
    "\n",
    "    @classmethod\n",
    "    def random_2D(\n",
    "        cls, nmodes: int, std: float, scale: float = 10.0, seed = 0.0\n",
    "    ) -> \"GaussianMixture\":\n",
    "        torch.manual_seed(seed)\n",
    "        means = (torch.rand(nmodes, 2) - 0.5) * scale\n",
    "        covs = torch.diag_embed(torch.ones(nmodes, 2)) * std ** 2\n",
    "        weights = torch.ones(nmodes)\n",
    "        return cls(means, covs, weights)\n",
    "\n",
    "    @classmethod\n",
    "    def symmetric_2D(\n",
    "        cls, nmodes: int, std: float, scale: float = 10.0,\n",
    "    ) -> \"GaussianMixture\":\n",
    "        angles = torch.linspace(0, 2 * np.pi, nmodes + 1)[:nmodes]\n",
    "        means = torch.stack([torch.cos(angles), torch.sin(angles)], dim=1) * scale\n",
    "        covs = torch.diag_embed(torch.ones(nmodes, 2) * std ** 2)\n",
    "        weights = torch.ones(nmodes) / nmodes\n",
    "        return cls(means, covs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce6533-2e56-42cb-98e7-958c45d583f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize densities\n",
    "densities = {\n",
    "    \"Gaussian\": Gaussian(mean=torch.zeros(2), cov=10 * torch.eye(2)).to(device),\n",
    "    \"Random Mixture\": GaussianMixture.random_2D(nmodes=5, std=1.0, scale=20.0, seed=3.0).to(device),\n",
    "    \"Symmetric Mixture\": GaussianMixture.symmetric_2D(nmodes=5, std=1.0, scale=8.0).to(device),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(18, 6))\n",
    "bins = 100\n",
    "scale = 15\n",
    "for idx, (name, density) in enumerate(densities.items()):\n",
    "    ax = axes[idx]\n",
    "    ax.set_title(name)\n",
    "    imshow_density(density, bins, scale, ax, vmin=-15, cmap=plt.get_cmap('Blues'))\n",
    "    contour_density(density, bins, scale, ax, colors='grey', linestyles='solid', alpha=0.25, levels=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51093e-b25a-4cdb-bc0d-ab8c1a3145b3",
   "metadata": {},
   "source": [
    "### Question 3.1: Implementing Langevin Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91888056-b900-401b-bebb-4e4bf5301afe",
   "metadata": {},
   "source": [
    "In this section, we'll simulate the (overdamped) Langevin dynamics $$dX_t = \\frac{1}{2} \\sigma^2\\nabla \\log p(X_t) dt + \\sigma dW_t.$$\n",
    "\n",
    "**Your job**: Fill in the `drift_coefficient` and `diffusion_coefficient` methods of the class `LangevinSDE` below.\n",
    "\n",
    "**Hint**: Use `Density.score` to access the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f00c5-3030-4f6c-945f-4a7d3483ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangevinSDE(SDE):\n",
    "    def __init__(self, sigma: float, density: Density):\n",
    "        self.sigma = sigma\n",
    "        self.density = density\n",
    "        \n",
    "    def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the drift coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (bs, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - drift: shape (bs, dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Fill me in for Question 3.1!\")\n",
    "        \n",
    "    def diffusion_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the diffusion coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (bs, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - diffusion: shape (bs, dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Fill me in for Question 3.1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2235a-befd-4f25-b0ac-849c9868cd5d",
   "metadata": {},
   "source": [
    "Now, let's graph the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7347543-533a-48c9-9c08-193898b139c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define two utility functions...\n",
    "def every_nth_index(num_timesteps: int, n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the indices to record in the trajectory\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return torch.arange(num_timesteps)\n",
    "    return torch.cat(\n",
    "        [\n",
    "            torch.arange(0, num_timesteps - 1, n),\n",
    "            torch.tensor([num_timesteps - 1]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def graph_dynamics(\n",
    "    num_samples: int,\n",
    "    source_distribution: Sampleable,\n",
    "    simulator: Simulator, \n",
    "    density: Density,\n",
    "    timesteps: torch.Tensor, \n",
    "    plot_every: int,\n",
    "    bins: int,\n",
    "    scale: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the evolution of samples from source under the simulation scheme given by simulator (itself a discretization of an ODE or SDE).\n",
    "    Args:\n",
    "        - num_samples: the number of samples to simulate\n",
    "        - source_distribution: distribution from which we draw initial samples at t=0\n",
    "        - simulator: the discertized simulation scheme used to simulate the dynamics\n",
    "        - density: the target density\n",
    "        - timesteps: the timesteps used by the simulator\n",
    "        - plot_every: number of timesteps between consecutive plots\n",
    "        - bins: number of bins for imshow\n",
    "        - scale: scale for imshow\n",
    "    \"\"\"\n",
    "    # Simulate\n",
    "    x0 = source_distribution.sample(num_samples)\n",
    "    xts = simulator.simulate_with_trajectory(x0, timesteps)\n",
    "    indices_to_plot = every_nth_index(len(timesteps), plot_every)\n",
    "    plot_timesteps = timesteps[indices_to_plot]\n",
    "    plot_xts = xts[:,indices_to_plot]\n",
    "\n",
    "    # Graph\n",
    "    fig, axes = plt.subplots(2, len(plot_timesteps), figsize=(8*len(plot_timesteps), 16))\n",
    "    axes = axes.reshape((2,len(plot_timesteps)))\n",
    "    for t_idx in range(len(plot_timesteps)):\n",
    "        t = plot_timesteps[t_idx].item()\n",
    "        xt = plot_xts[:,t_idx]\n",
    "        # Scatter axes\n",
    "        scatter_ax = axes[0, t_idx]\n",
    "        imshow_density(density, bins, scale, scatter_ax, vmin=-15, alpha=0.25, cmap=plt.get_cmap('Blues'))\n",
    "        scatter_ax.scatter(xt[:,0].cpu(), xt[:,1].cpu(), marker='x', color='black', alpha=0.75, s=15)\n",
    "        scatter_ax.set_title(f'Samples at t={t:.1f}', fontsize=15)\n",
    "        scatter_ax.set_xticks([])\n",
    "        scatter_ax.set_yticks([])\n",
    "\n",
    "        # Kdeplot axes\n",
    "        kdeplot_ax = axes[1, t_idx]\n",
    "        imshow_density(density, bins, scale, kdeplot_ax, vmin=-15, alpha=0.5, cmap=plt.get_cmap('Blues'))\n",
    "        sns.kdeplot(x=xt[:,0].cpu(), y=xt[:,1].cpu(), alpha=0.5, ax=kdeplot_ax,color='grey')\n",
    "        kdeplot_ax.set_title(f'Density of Samples at t={t:.1f}', fontsize=15)\n",
    "        kdeplot_ax.set_xticks([])\n",
    "        kdeplot_ax.set_yticks([])\n",
    "        kdeplot_ax.set_xlabel(\"\")\n",
    "        kdeplot_ax.set_ylabel(\"\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b244a6-5b25-4b83-a4fb-ff14c29d5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the simulator\n",
    "target = GaussianMixture.random_2D(nmodes=5, std=0.75, scale=15.0, seed=3.0).to(device)\n",
    "sde = LangevinSDE(sigma = 0.6, density = target)\n",
    "simulator = EulerMaruyamaSimulator(sde)\n",
    "\n",
    "# Graph the results!\n",
    "graph_dynamics(\n",
    "    num_samples = 1000,\n",
    "    source_distribution = Gaussian(mean=torch.zeros(2), cov=20 * torch.eye(2)).to(device),\n",
    "    simulator=simulator,\n",
    "    density=target,\n",
    "    timesteps=torch.linspace(0,5.0,1000).to(device),\n",
    "    plot_every=334,\n",
    "    bins=200,\n",
    "    scale=15\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d75ef-806a-4012-ae59-6cc7faa5eaf1",
   "metadata": {},
   "source": [
    "**Your job**: Try varying the value of $\\sigma$, the number and range of the simulation steps, the source distribution, and target density. What do you notice? Why?\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d552e-0d1d-4cae-b5e6-23d270b7193f",
   "metadata": {},
   "source": [
    "Note: To run the folowing two **optional** cells, you will need to download the `ffmpeg` library. You can do so using e.g., `conda install -c conda-forge ffmpeg` (or, ideally, `mamba`). Running `pip install ffmpeg` or similar will likely **not** work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a53ea-d24e-4113-a190-e53d661bacec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "\n",
    "def animate_dynamics(\n",
    "    num_samples: int,\n",
    "    source_distribution: Sampleable,\n",
    "    simulator: Simulator, \n",
    "    density: Density,\n",
    "    timesteps: torch.Tensor, \n",
    "    animate_every: int,\n",
    "    bins: int,\n",
    "    scale: float,\n",
    "    save_path: str = 'dynamics_animation.mp4'\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the evolution of samples from source under the simulation scheme given by simulator (itself a discretization of an ODE or SDE).\n",
    "    Args:\n",
    "        - num_samples: the number of samples to simulate\n",
    "        - source_distribution: distribution from which we draw initial samples at t=0\n",
    "        - simulator: the discertized simulation scheme used to simulate the dynamics\n",
    "        - density: the target density\n",
    "        - timesteps: the timesteps used by the simulator\n",
    "        - animate_every: number of timesteps between consecutive frames in the resulting animation\n",
    "    \"\"\"\n",
    "    # Simulate\n",
    "    x0 = source_distribution.sample(num_samples)\n",
    "    xts = simulator.simulate_with_trajectory(x0, timesteps)\n",
    "    indices_to_animate = every_nth_index(len(timesteps), animate_every)\n",
    "    animate_timesteps = timesteps[indices_to_animate]\n",
    "    animate_xts = xts[:, indices_to_animate]\n",
    "\n",
    "    # Graph\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    camera = Camera(fig)\n",
    "    for t_idx in range(len(animate_timesteps)):\n",
    "        t = animate_timesteps[t_idx].item()\n",
    "        xt = animate_xts[:,t_idx]\n",
    "        # Scatter axes\n",
    "        scatter_ax = axes[0]\n",
    "        imshow_density(density, bins, scale, scatter_ax, vmin=-15, alpha=0.25, cmap=plt.get_cmap('Blues'))\n",
    "        scatter_ax.scatter(xt[:,0].cpu(), xt[:,1].cpu(), marker='x', color='black', alpha=0.75, s=15)\n",
    "        scatter_ax.set_title(f'Samples')\n",
    "\n",
    "        # Kdeplot axes\n",
    "        kdeplot_ax = axes[1]\n",
    "        imshow_density(density, bins, scale, kdeplot_ax, vmin=-15, alpha=0.5, cmap=plt.get_cmap('Blues'))\n",
    "        sns.kdeplot(x=xt[:,0].cpu(), y=xt[:,1].cpu(), alpha=0.5, ax=kdeplot_ax,color='grey')\n",
    "        kdeplot_ax.set_title(f'Density of Samples', fontsize=15)\n",
    "        kdeplot_ax.set_xticks([])\n",
    "        kdeplot_ax.set_yticks([])\n",
    "        kdeplot_ax.set_xlabel(\"\")\n",
    "        kdeplot_ax.set_ylabel(\"\")\n",
    "        camera.snap()\n",
    "    \n",
    "    animation = camera.animate()\n",
    "    animation.save(save_path)\n",
    "    plt.close()\n",
    "    return HTML(animation.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4269e-9e6d-4a50-8353-3b380eae4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL CELL\n",
    "# Construct the simulator\n",
    "target = GaussianMixture.random_2D(nmodes=5, std=0.75, scale=15.0, seed=3.0).to(device)\n",
    "sde = LangevinSDE(sigma = 0.6, density = target)\n",
    "simulator = EulerMaruyamaSimulator(sde)\n",
    "\n",
    "# Graph the results!\n",
    "animate_dynamics(\n",
    "    num_samples = 1000,\n",
    "    source_distribution = Gaussian(mean=torch.zeros(2), cov=20 * torch.eye(2)).to(device),\n",
    "    simulator=simulator,\n",
    "    density=target,\n",
    "    timesteps=torch.linspace(0,5.0,1000).to(device),\n",
    "    bins=200,\n",
    "    scale=15,\n",
    "    animate_every=100\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149c323-3c9e-40a7-8e00-15fe8b87f3f8",
   "metadata": {},
   "source": [
    "### Question 3.2: Ornstein-Uhlenbeck as Langevin Dynamics\n",
    "In this section, we'll finish off with a brief mathematical exercise connecting Langevin dynamics and Ornstein-Uhlenbeck processes. Recall that for (suitably nice) distribution $p$, the *Langevin dynamics* are given by\n",
    "$$dX_t = \\frac{1}{2} \\sigma^2\\nabla \\log p(X_t) dt + \\sigma\\, dW_t, \\quad \\quad X_0 = x_0,$$\n",
    "while for given $\\theta, \\sigma$, the Ornstein-Uhlenbeck process is given by\n",
    "$$dX_t = -\\theta X_t\\, dt + \\sigma\\, dW_t, \\quad \\quad X_0 = x_0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86954c67-510b-4d10-aea1-b5636f4dbb47",
   "metadata": {},
   "source": [
    "**Your job**: Show that when $p(x) = N(0, \\frac{\\sigma^2}{2\\theta})$, the score is given by $$\\nabla \\log p(x) = -\\frac{2\\theta}{\\sigma^2}x.$$\n",
    "\n",
    "**Hint**: The probability density of the Gaussian $p(x) = N(0, \\frac{\\sigma^2}{2\\theta})$ is given by $$p(x)  = \\frac{\\sqrt{\\theta}}{\\sigma\\sqrt{\\pi}} \\exp\\left(-\\frac{x^2\\theta}{\\sigma^2}\\right).$$\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a0f761-c85c-4e30-b497-bc7622bc8e72",
   "metadata": {},
   "source": [
    "**Your job**: Conclude that when $p(x) = N(0, \\frac{\\sigma^2}{2\\theta})$, the Langevin dynamics \n",
    "$$dX_t = \\frac{1}{2} \\sigma^2\\nabla \\log p(X_t) dt + \\sigma dW_t,$$\n",
    "is equivalent to the Ornstein-Uhlenbeck process\n",
    "$$ dX_t = -\\theta X_t\\, dt + \\sigma\\, dW_t, \\quad \\quad X_0 = 0.$$\n",
    "\n",
    "**Your answer**:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtds",
   "language": "python",
   "name": "mtds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
